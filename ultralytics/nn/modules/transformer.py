# Ultralytics YOLO ğŸš€, AGPL-3.0 license
"""Transformer modules."""

import math

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.init import constant_, xavier_uniform_

from .conv import Conv
from .position_encoding import Conv2dNormActivation,get_sine_pos_embed
from .utils import _get_clones, inverse_sigmoid, multi_scale_deformable_attn_pytorch

__all__ = ('TransformerEncoderLayer', 'TransformerLayer', 'TransformerBlock', 'MLPBlock', 'LayerNorm2d', 'AIFI',
           'DeformableTransformerDecoder', 'DeformableTransformerDecoderLayer', 'MSDeformAttn', 'MLP', 'PositionRelationEmbedding')


class TransformerEncoderLayer(nn.Module):
    """Defines a single layer of the transformer encoder."""

    def __init__(self, c1, cm=2048, num_heads=8, dropout=0.0, act=nn.GELU(), normalize_before=False):
        """Initialize the TransformerEncoderLayer with specified parameters."""
        super().__init__()
        from ...utils.torch_utils import TORCH_1_9
        if not TORCH_1_9:
            raise ModuleNotFoundError(
                'TransformerEncoderLayer() requires torch>=1.9 to use nn.MultiheadAttention(batch_first=True).')
        self.ma = nn.MultiheadAttention(c1, num_heads, dropout=dropout, batch_first=True)
        # Implementation of Feedforward model
        self.fc1 = nn.Linear(c1, cm)
        self.fc2 = nn.Linear(cm, c1)

        self.norm1 = nn.LayerNorm(c1)
        self.norm2 = nn.LayerNorm(c1)
        self.dropout = nn.Dropout(dropout)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)

        self.act = act
        self.normalize_before = normalize_before

    @staticmethod
    def with_pos_embed(tensor, pos=None):
        """Add position embeddings to the tensor if provided."""
        return tensor if pos is None else tensor + pos

    def forward_post(self, src, src_mask=None, src_key_padding_mask=None, pos=None):
        """Performs forward pass with post-normalization."""
        q = k = self.with_pos_embed(src, pos)
        src2 = self.ma(q, k, value=src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)[0]
        src = src + self.dropout1(src2)
        src = self.norm1(src)
        src2 = self.fc2(self.dropout(self.act(self.fc1(src))))
        src = src + self.dropout2(src2)
        return self.norm2(src)

    def forward_pre(self, src, src_mask=None, src_key_padding_mask=None, pos=None):
        """Performs forward pass with pre-normalization."""
        src2 = self.norm1(src)
        q = k = self.with_pos_embed(src2, pos)
        src2 = self.ma(q, k, value=src2, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)[0]
        src = src + self.dropout1(src2)
        src2 = self.norm2(src)
        src2 = self.fc2(self.dropout(self.act(self.fc1(src2))))
        return src + self.dropout2(src2)

    def forward(self, src, src_mask=None, src_key_padding_mask=None, pos=None):
        """Forward propagates the input through the encoder module."""
        if self.normalize_before:
            return self.forward_pre(src, src_mask, src_key_padding_mask, pos)
        return self.forward_post(src, src_mask, src_key_padding_mask, pos)


class AIFI(TransformerEncoderLayer):
    """Defines the AIFI transformer layer."""

    def __init__(self, c1, cm=2048, num_heads=8, dropout=0, act=nn.GELU(), normalize_before=False):
        """Initialize the AIFI instance with specified parameters."""
        super().__init__(c1, cm, num_heads, dropout, act, normalize_before)

    def forward(self, x):
        """Forward pass for the AIFI transformer layer."""
        c, h, w = x.shape[1:]
        pos_embed = self.build_2d_sincos_position_embedding(w, h, c)
        # Flatten [B, C, H, W] to [B, HxW, C]
        x = super().forward(x.flatten(2).permute(0, 2, 1), pos=pos_embed.to(device=x.device, dtype=x.dtype))
        return x.permute(0, 2, 1).view([-1, c, h, w]).contiguous()

    @staticmethod
    def build_2d_sincos_position_embedding(w, h, embed_dim=256, temperature=10000.0):
        """Builds 2D sine-cosine position embedding."""
        grid_w = torch.arange(int(w), dtype=torch.float32)
        grid_h = torch.arange(int(h), dtype=torch.float32)
        grid_w, grid_h = torch.meshgrid(grid_w, grid_h, indexing='ij')
        assert embed_dim % 4 == 0, \
            'Embed dimension must be divisible by 4 for 2D sin-cos position embedding'
        pos_dim = embed_dim // 4
        omega = torch.arange(pos_dim, dtype=torch.float32) / pos_dim
        omega = 1. / (temperature ** omega)

        out_w = grid_w.flatten()[..., None] @ omega[None]
        out_h = grid_h.flatten()[..., None] @ omega[None]

        return torch.cat([torch.sin(out_w), torch.cos(out_w), torch.sin(out_h), torch.cos(out_h)], 1)[None]

class TransformerLayer(nn.Module):
    """Transformer layer https://arxiv.org/abs/2010.11929 (LayerNorm layers removed for better performance)."""

    def __init__(self, c, num_heads):
        """Initializes a self-attention mechanism using linear transformations and multi-head attention."""
        super().__init__()
        self.q = nn.Linear(c, c, bias=False)
        self.k = nn.Linear(c, c, bias=False)
        self.v = nn.Linear(c, c, bias=False)
        self.ma = nn.MultiheadAttention(embed_dim=c, num_heads=num_heads)
        self.fc1 = nn.Linear(c, c, bias=False)
        self.fc2 = nn.Linear(c, c, bias=False)

    def forward(self, x):
        """Apply a transformer block to the input x and return the output."""
        x = self.ma(self.q(x), self.k(x), self.v(x))[0] + x
        return self.fc2(self.fc1(x)) + x


class TransformerBlock(nn.Module):
    """Vision Transformer https://arxiv.org/abs/2010.11929."""

    def __init__(self, c1, c2, num_heads, num_layers):
        """Initialize a Transformer module with position embedding and specified number of heads and layers."""
        super().__init__()
        self.conv = None
        if c1 != c2:
            self.conv = Conv(c1, c2)
        self.linear = nn.Linear(c2, c2)  # learnable position embedding
        self.tr = nn.Sequential(*(TransformerLayer(c2, num_heads) for _ in range(num_layers)))
        self.c2 = c2

    def forward(self, x):
        """Forward propagates the input through the bottleneck module."""
        if self.conv is not None:
            x = self.conv(x)
        b, _, w, h = x.shape
        p = x.flatten(2).permute(2, 0, 1)
        return self.tr(p + self.linear(p)).permute(1, 2, 0).reshape(b, self.c2, w, h)


class MLPBlock(nn.Module):
    """Implements a single block of a multi-layer perceptron."""

    def __init__(self, embedding_dim, mlp_dim, act=nn.GELU):
        """Initialize the MLPBlock with specified embedding dimension, MLP dimension, and activation function."""
        super().__init__()
        self.lin1 = nn.Linear(embedding_dim, mlp_dim)
        self.lin2 = nn.Linear(mlp_dim, embedding_dim)
        self.act = act()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass for the MLPBlock."""
        return self.lin2(self.act(self.lin1(x)))


class MLP(nn.Module):
    """Implements a simple multi-layer perceptron (also called FFN)."""

    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):
        """Initialize the MLP with specified input, hidden, output dimensions and number of layers."""
        super().__init__()
        self.num_layers = num_layers
        h = [hidden_dim] * (num_layers - 1)
        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))

    def forward(self, x):
        """Forward pass for the entire MLP."""
        for i, layer in enumerate(self.layers):
            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)
        return x

class FourierMLP(nn.Module):
    def __init__(self, input_dim=6, hidden_dim=256, output_dim=256, sigma=10.0):
        """
        Args:
            input_dim: è¾“å…¥ç‰¹å¾ç»´åº¦ (æˆ‘ä»¬æ˜¯ 7: 1 GCD + 2 Angle + 4 LogRel)
            sigma: æ§åˆ¶é¢‘ç‡åˆ†å¸ƒçš„æ ‡å‡†å·®ã€‚
                   sigma è¶Šå¤§ï¼Œå¯¹é«˜é¢‘ç»†èŠ‚(å¯†é›†å°ç›®æ ‡)è¶Šæ•æ„Ÿï¼›
                   sigma è¶Šå°ï¼Œå¯¹ä½é¢‘(æ•´ä½“å¸ƒå±€)è¶Šæ•æ„Ÿã€‚
                   å¯¹äº UAV å¯†é›†åœºæ™¯ï¼Œå»ºè®®è®¾ä¸º 10.0 - 30.0ã€‚
        """
        super().__init__()
        # 1. éšæœºé«˜æ–¯çŸ©é˜µ B (ä¸å¯å­¦ä¹ ï¼Œç±»ä¼¼ä½ç½®ç¼–ç çš„åŸºåº•)
        # æ˜ å°„åˆ° hidden_dim çš„ä¸€åŠï¼Œå› ä¸ºåé¢ä¼šæœ‰ sin å’Œ cos æ‹¼èµ·æ¥
        self.mapping_size = hidden_dim // 2
        self.register_buffer('B', torch.randn(input_dim, self.mapping_size) * sigma)
        
        # 2. åç»­çš„ MLP
        self.mlp = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim), # è¾“å…¥æ˜¯ sin + cos
            nn.GELU(),
            nn.Linear(hidden_dim, output_dim)
        )

    def forward(self, x):
        # x: [BS, N, M, input_dim]
        
        # 1. å‚…é‡Œå¶æ˜ å°„ (Fourier Mapping)
        # v -> [sin(2*pi*B*v), cos(2*pi*B*v)]
        # è¿™ä¸€æ­¥æŠŠä½ç»´åæ ‡æ˜ å°„åˆ°äº†é«˜ç»´æµå½¢ï¼Œä¸”æ¶ˆé™¤äº†å‘¨æœŸæ€§æ··æ·†
        projected = (2 * torch.pi * x) @ self.B
        x_fourier = torch.cat([torch.sin(projected), torch.cos(projected)], dim=-1)
        
        # 2. MLP å¤„ç†
        return self.mlp(x_fourier)

class LayerNorm2d(nn.Module):
    """
    2D Layer Normalization module inspired by Detectron2 and ConvNeXt implementations.

    Original implementations in
    https://github.com/facebookresearch/detectron2/blob/main/detectron2/layers/batch_norm.py
    and
    https://github.com/facebookresearch/ConvNeXt/blob/main/models/convnext.py.
    """

    def __init__(self, num_channels, eps=1e-6):
        """Initialize LayerNorm2d with the given parameters."""
        super().__init__()
        self.weight = nn.Parameter(torch.ones(num_channels))
        self.bias = nn.Parameter(torch.zeros(num_channels))
        self.eps = eps

    def forward(self, x):
        """Perform forward pass for 2D layer normalization."""
        u = x.mean(1, keepdim=True)
        s = (x - u).pow(2).mean(1, keepdim=True)
        x = (x - u) / torch.sqrt(s + self.eps)
        return self.weight[:, None, None] * x + self.bias[:, None, None]


class MSDeformAttn(nn.Module):
    """
    Multi-Scale Deformable Attention Module based on Deformable-DETR and PaddleDetection implementations.

    https://github.com/fundamentalvision/Deformable-DETR/blob/main/models/ops/modules/ms_deform_attn.py
    """

    def __init__(self, d_model=256, n_levels=4, n_heads=8, n_points=4):
        """Initialize MSDeformAttn with the given parameters."""
        super().__init__()
        if d_model % n_heads != 0:
            raise ValueError(f'd_model must be divisible by n_heads, but got {d_model} and {n_heads}')
        _d_per_head = d_model // n_heads
        # Better to set _d_per_head to a power of 2 which is more efficient in a CUDA implementation
        assert _d_per_head * n_heads == d_model, '`d_model` must be divisible by `n_heads`'

        self.im2col_step = 64

        self.d_model = d_model
        self.n_levels = n_levels
        self.n_heads = n_heads
        self.n_points = n_points

        self.sampling_offsets = nn.Linear(d_model, n_heads * n_levels * n_points * 2)
        self.attention_weights = nn.Linear(d_model, n_heads * n_levels * n_points)
        self.value_proj = nn.Linear(d_model, d_model)
        self.output_proj = nn.Linear(d_model, d_model)

        self._reset_parameters()

    def _reset_parameters(self):
        """Reset module parameters."""
        constant_(self.sampling_offsets.weight.data, 0.)
        thetas = torch.arange(self.n_heads, dtype=torch.float32) * (2.0 * math.pi / self.n_heads)
        grid_init = torch.stack([thetas.cos(), thetas.sin()], -1)
        grid_init = (grid_init / grid_init.abs().max(-1, keepdim=True)[0]).view(self.n_heads, 1, 1, 2).repeat(
            1, self.n_levels, self.n_points, 1)
        for i in range(self.n_points):
            grid_init[:, :, i, :] *= i + 1
        with torch.no_grad():
            self.sampling_offsets.bias = nn.Parameter(grid_init.view(-1))
        constant_(self.attention_weights.weight.data, 0.)
        constant_(self.attention_weights.bias.data, 0.)
        xavier_uniform_(self.value_proj.weight.data)
        constant_(self.value_proj.bias.data, 0.)
        xavier_uniform_(self.output_proj.weight.data)
        constant_(self.output_proj.bias.data, 0.)

    def forward(self, query, refer_bbox, value, value_shapes, value_mask=None):
        """
        Perform forward pass for multiscale deformable attention.

        https://github.com/PaddlePaddle/PaddleDetection/blob/develop/ppdet/modeling/transformers/deformable_transformer.py

        Args:
            query (torch.Tensor): [bs, query_length, C]
            refer_bbox (torch.Tensor): [bs, query_length, n_levels, 2], range in [0, 1], top-left (0,0),
                bottom-right (1, 1), including padding area
            value (torch.Tensor): [bs, value_length, C]
            value_shapes (List): [n_levels, 2], [(H_0, W_0), (H_1, W_1), ..., (H_{L-1}, W_{L-1})]
            value_mask (Tensor): [bs, value_length], True for non-padding elements, False for padding elements

        Returns:
            output (Tensor): [bs, Length_{query}, C]
        """
        bs, len_q = query.shape[:2]
        len_v = value.shape[1]
        assert sum(s[0] * s[1] for s in value_shapes) == len_v

        value = self.value_proj(value)
        if value_mask is not None:
            value = value.masked_fill(value_mask[..., None], float(0))
        value = value.view(bs, len_v, self.n_heads, self.d_model // self.n_heads)
        sampling_offsets = self.sampling_offsets(query).view(bs, len_q, self.n_heads, self.n_levels, self.n_points, 2)
        attention_weights = self.attention_weights(query).view(bs, len_q, self.n_heads, self.n_levels * self.n_points)
        attention_weights = F.softmax(attention_weights, -1).view(bs, len_q, self.n_heads, self.n_levels, self.n_points)
        # N, Len_q, n_heads, n_levels, n_points, 2
        num_points = refer_bbox.shape[-1]
        if num_points == 2:
            offset_normalizer = torch.as_tensor(value_shapes, dtype=query.dtype, device=query.device).flip(-1)
            add = sampling_offsets / offset_normalizer[None, None, None, :, None, :]
            sampling_locations = refer_bbox[:, :, None, :, None, :] + add
        elif num_points == 4:
            add = sampling_offsets / self.n_points * refer_bbox[:, :, None, :, None, 2:] * 0.5
            sampling_locations = refer_bbox[:, :, None, :, None, :2] + add
        else:
            raise ValueError(f'Last dim of reference_points must be 2 or 4, but got {num_points}.')
        output = multi_scale_deformable_attn_pytorch(value, value_shapes, sampling_locations, attention_weights)
        return self.output_proj(output)


class DeformableTransformerDecoderLayer(nn.Module):
    """
    Deformable Transformer Decoder Layer inspired by PaddleDetection and Deformable-DETR implementations.

    https://github.com/PaddlePaddle/PaddleDetection/blob/develop/ppdet/modeling/transformers/deformable_transformer.py
    https://github.com/fundamentalvision/Deformable-DETR/blob/main/models/deformable_transformer.py
    """

    def __init__(self, d_model=256, n_heads=8, d_ffn=1024, dropout=0., act=nn.ReLU(), n_levels=4, n_points=4):
        """Initialize the DeformableTransformerDecoderLayer with the given parameters."""
        super().__init__()
        self.num_heads = n_heads

        # Self attention
        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)
        self.dropout1 = nn.Dropout(dropout)
        self.norm1 = nn.LayerNorm(d_model)

        # Cross attention
        self.cross_attn = MSDeformAttn(d_model, n_levels, n_heads, n_points)
        self.dropout2 = nn.Dropout(dropout)
        self.norm2 = nn.LayerNorm(d_model)

        # FFN
        self.linear1 = nn.Linear(d_model, d_ffn)
        self.act = act
        self.dropout3 = nn.Dropout(dropout)
        self.linear2 = nn.Linear(d_ffn, d_model)
        self.dropout4 = nn.Dropout(dropout)
        self.norm3 = nn.LayerNorm(d_model)

    @staticmethod
    def with_pos_embed(tensor, pos):
        """Add positional embeddings to the input tensor, if provided."""
        return tensor if pos is None else tensor + pos

    def forward_ffn(self, tgt):
        """Perform forward pass through the Feed-Forward Network part of the layer."""
        tgt2 = self.linear2(self.dropout3(self.act(self.linear1(tgt))))
        tgt = tgt + self.dropout4(tgt2)
        return self.norm3(tgt)

    def forward(self, embed, refer_bbox, feats, shapes, padding_mask=None, attn_mask=None, query_pos=None):
        """Perform the forward pass through the entire decoder layer."""

        # Self attention
        q = k = self.with_pos_embed(embed, query_pos)
        tgt = self.self_attn(q.transpose(0, 1), k.transpose(0, 1), embed.transpose(0, 1),
                             attn_mask=attn_mask)[0].transpose(0, 1)
        embed = embed + self.dropout1(tgt)
        embed = self.norm1(embed)

        # Cross attention
        tgt = self.cross_attn(self.with_pos_embed(embed, query_pos), refer_bbox.unsqueeze(2), feats, shapes,
                              padding_mask)
        embed = embed + self.dropout2(tgt)
        embed = self.norm2(embed)

        # FFN
        return self.forward_ffn(embed)


class DeformableTransformerDecoder(nn.Module):
    """
    Implementation of Deformable Transformer Decoder based on PaddleDetection.

    https://github.com/PaddlePaddle/PaddleDetection/blob/develop/ppdet/modeling/transformers/deformable_transformer.py
    """

    def __init__(self, hidden_dim, decoder_layer, num_layers, eval_idx=-1):
        """Initialize the DeformableTransformerDecoder with the given parameters."""
        super().__init__()
        self.layers = _get_clones(decoder_layer, num_layers)
        self.num_layers = num_layers
        self.hidden_dim = hidden_dim
        self.eval_idx = eval_idx if eval_idx >= 0 else num_layers + eval_idx

    def forward(
            self,
            embed,  # decoder embeddings
            refer_bbox,  # anchor
            feats,  # image features
            shapes,  # feature shapes
            bbox_head,
            score_head,
            pos_mlp,
            attn_mask=None,
            padding_mask=None):
        """Perform the forward pass through the entire decoder."""
        output = embed
        dec_bboxes = []
        dec_cls = []
        last_refined_bbox = None
        refer_bbox = refer_bbox.sigmoid()
        for i, layer in enumerate(self.layers):
            output = layer(output, refer_bbox, feats, shapes, padding_mask, attn_mask, pos_mlp(refer_bbox))

            bbox = bbox_head[i](output)
            refined_bbox = torch.sigmoid(bbox + inverse_sigmoid(refer_bbox))

            if self.training:
                dec_cls.append(score_head[i](output))
                if i == 0:
                    dec_bboxes.append(refined_bbox)
                else:
                    dec_bboxes.append(torch.sigmoid(bbox + inverse_sigmoid(last_refined_bbox)))
            elif i == self.eval_idx:
                dec_cls.append(score_head[i](output))
                dec_bboxes.append(refined_bbox)
                break

            last_refined_bbox = refined_bbox
            refer_bbox = refined_bbox.detach() if self.training else refined_bbox

        return torch.stack(dec_bboxes), torch.stack(dec_cls)


class Deformable_position_TransformerDecoder(nn.Module):
    """
    Implementation of Deformable Transformer Decoder based on PaddleDetection.

    https://github.com/PaddlePaddle/PaddleDetection/blob/develop/ppdet/modeling/transformers/deformable_transformer.py
    """

    def __init__(self, hidden_dim, decoder_layer, num_layers, eval_idx=-1):
        """Initialize the DeformableTransformerDecoder with the given parameters."""
        super().__init__()
        self.layers = _get_clones(decoder_layer, num_layers)
        self.num_layers = num_layers
        self.hidden_dim = hidden_dim
        self.num_heads = decoder_layer.num_heads
        self.eval_idx = eval_idx if eval_idx >= 0 else num_layers + eval_idx
        self.position_relation_embedding = PositionRelationEmbedding(4, self.num_heads) 

    def forward(
            self,
            embed,  # decoder embeddings
            refer_bbox,  # anchor
            feats,  # image features
            shapes,  # feature shapes
            bbox_head,
            score_head,
            pos_mlp,
            attn_mask=None,
            padding_mask=None):
        """Perform the forward pass through the entire decoder."""
        output = embed
        dec_bboxes = []
        dec_cls = []
        last_refined_bbox = None
        refer_bbox = refer_bbox.sigmoid()
        pos_relation = attn_mask  # fallback pos_relation to attn_mask
        for i, layer in enumerate(self.layers):
            output = layer(output, refer_bbox, feats, shapes, padding_mask, pos_relation, pos_mlp(refer_bbox))
            
            bbox = bbox_head[i](output)
            refined_bbox = torch.sigmoid(bbox + inverse_sigmoid(refer_bbox))
            if i>0:
                dec_bbox = torch.sigmoid(bbox + inverse_sigmoid(last_refined_bbox))

            if self.training:
                dec_cls.append(score_head[i](output))
                if i == 0:
                    dec_bboxes.append(refined_bbox)
                else:

                    dec_bboxes.append(dec_bbox)
            elif i == self.eval_idx:
                dec_cls.append(score_head[i](output))
                dec_bboxes.append(refined_bbox)
                break
            
            if i == self.num_layers - 1:
                break
            src_boxes = tgt_boxes if i >=1 else refer_bbox
            tgt_boxes = refined_bbox if i==0 else dec_bbox
            pos_relation = self.position_relation_embedding(src_boxes,tgt_boxes).flatten(0,1)

            if attn_mask is not None:
                pos_relation.masked_fill_(attn_mask,float("-inf"))
            
            last_refined_bbox = refined_bbox
            refer_bbox = refined_bbox.detach() if self.training else refined_bbox

        return torch.stack(dec_bboxes), torch.stack(dec_cls)

    

# ================= Variation B: Original (Relation-DETR) =================
def box_rel_encoding(src_boxes, tgt_boxes, eps=1e-5):
    """
    è¾“å‡ºç»´åº¦: 4 (delta_x, delta_y, delta_w, delta_h)
    """
    xy1, wh1 = src_boxes.split([2, 2], -1)
    xy2, wh2 = tgt_boxes.split([2, 2], -1)
    
    delta_xy = torch.abs(xy1.unsqueeze(-2) - xy2.unsqueeze(-3))
    delta_xy = torch.log(delta_xy / (wh1.unsqueeze(-2) + eps) + 1.0)
    
    delta_wh = torch.log((wh1.unsqueeze(-2) + eps) / (wh2.unsqueeze(-3) + eps))
    
    pos_embed = torch.cat([delta_xy, delta_wh], -1) 
    return pos_embed

# ================= Variation C: Original + IoU =================
def iou_rel_encoding(src_boxes, tgt_boxes, eps=1e-5):
    """
    è¾“å‡ºç»´åº¦: 5 (Original 4 dims + 1 IoU)
    """
    # 1. è®¡ç®—åŸºç¡€ç‰¹å¾ (Variation B)
    base_embed = box_rel_encoding(src_boxes, tgt_boxes, eps)
    
    # 2. è®¡ç®— IoU
    xy1, wh1 = src_boxes.split([2, 2], -1)
    xy2, wh2 = tgt_boxes.split([2, 2], -1)
    
    x1y1_1, x2y2_1 = xy1 - wh1/2, xy1 + wh1/2
    x1y1_2, x2y2_2 = xy2 - wh2/2, xy2 + wh2/2
    
    # å¹¿æ’­æœºåˆ¶: [BS, N, 1, 2] vs [BS, 1, M, 2]
    x1y1_inter = torch.maximum(x1y1_1.unsqueeze(-2), x1y1_2.unsqueeze(-3))
    x2y2_inter = torch.minimum(x2y2_1.unsqueeze(-2), x2y2_2.unsqueeze(-3))
    
    wh_inter = torch.clamp(x2y2_inter - x1y1_inter, min=0)
    area_inter = wh_inter.prod(dim=-1)
    
    area_1 = wh1.prod(dim=-1).unsqueeze(-1)
    area_2 = wh2.prod(dim=-1).unsqueeze(-2)
    
    iou = area_inter / (area_1 + area_2 - area_inter + eps)
    
    # 3. æ‹¼æ¥
    pos_embed = torch.cat([base_embed, iou.unsqueeze(-1)], -1)
    return pos_embed

# def box_rel_encoding(src_boxes, tgt_boxes, eps=1e-5):
#     # construct position relation
#     xy1, wh1 = src_boxes.split([2, 2], -1)
#     xy2, wh2 = tgt_boxes.split([2, 2], -1)
#     delta_xy = torch.abs(xy1.unsqueeze(-2) - xy2.unsqueeze(-3))
#     delta_xy = torch.log(delta_xy / (wh1.unsqueeze(-2) + eps) + 1.0)
#     delta_wh = torch.log((wh1.unsqueeze(-2) + eps) / (wh2.unsqueeze(-3) + eps))
#     # è®¡ç®—IoU
#     x1y1_1, x2y2_1 = xy1 - wh1/2, xy1 + wh1/2
#     x1y1_2, x2y2_2 = xy2 - wh2/2, xy2 + wh2/2
    
#     x1y1_inter = torch.maximum(x1y1_1.unsqueeze(-2), x1y1_2.unsqueeze(-3))
#     x2y2_inter = torch.minimum(x2y2_1.unsqueeze(-2), x2y2_2.unsqueeze(-3))
    
#     wh_inter = torch.clamp(x2y2_inter - x1y1_inter, min=0)
#     area_inter = wh_inter.prod(dim=-1)
    
#     area_1 = wh1.prod(dim=-1).unsqueeze(-1)
#     area_2 = wh2.prod(dim=-1).unsqueeze(-2)
    
#     iou = area_inter / (area_1 + area_2 - area_inter + eps)
    
#     # è®¡ç®—ç›¸å¯¹è·ç¦»
#     center1 = xy1
#     center2 = xy2
#     # rel_distance = torch.norm(center1.unsqueeze(-2) - center2.unsqueeze(-3), dim=-1)
#     # rel_distance = torch.log(rel_distance / torch.sqrt(wh1.prod(dim=-1)).unsqueeze(-1) + eps)
    
#     # è®¡ç®—ç›¸å¯¹è§’åº¦
#     delta_center = center2.unsqueeze(-3) - center1.unsqueeze(-2)
#     angle = torch.atan2(delta_center[..., 1], delta_center[..., 0])
    
#     # ç»„åˆæ‰€æœ‰ç‰¹å¾
#     pos_embed = torch.cat([
#         delta_xy,  # ç›¸å¯¹ä½ç½® (2)
#         delta_wh,  # ç›¸å¯¹å°ºå¯¸ (2)
#         iou.unsqueeze(-1),  # IoU (1)
#         # rel_distance.unsqueeze(-1),  # ç›¸å¯¹è·ç¦» (1)
#         angle.unsqueeze(-1),  # ç›¸å¯¹è§’åº¦ (1)
#     ], -1)
#     # pos_embed = torch.cat([delta_xy, delta_wh], -1)  # [batch_size, num_boxes1, num_boxes2, 4]

#     return pos_embed

def gaussian_relation_encoding(src_boxes, tgt_boxes, eps=1e-5):
    """
    åŸºäºé«˜æ–¯ Wasserstein è·ç¦»çš„ä¸ç¡®å®šæ€§å…³ç³»ç¼–ç  (Uncertainty-Aware Gaussian Relation)
    åˆ›æ–°ç‚¹: å°† BBox è§†ä¸º 2D é«˜æ–¯åˆ†å¸ƒï¼Œä½¿ç”¨ Wasserstein è·ç¦»è¡¡é‡å…¶ç›¸ä¼¼åº¦ï¼Œ
           è§£å†³å°ç›®æ ‡ IoU æ•æ„Ÿå’Œè¿œè·ç¦»æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚
    
    Args:
        src_boxes (Tensor): [BS, N, 4] (cx, cy, w, h), å€¼åŸŸé€šå¸¸åœ¨ 0-1 ä¹‹é—´
        tgt_boxes (Tensor): [BS, M, 4] (cx, cy, w, h)
    """
    # 1. ç»´åº¦æ‰©å±•ä»¥æ„å»ºä¸¤ä¸¤é…å¯¹çŸ©é˜µ (Pairwise Matrix)
    # src: [BS, N, 1, 4], tgt: [BS, 1, M, 4]
    if src_boxes.dim() == 3:
        b1 = src_boxes.unsqueeze(2)  
        b2 = tgt_boxes.unsqueeze(1)
    else:
        # å¤„ç†å¯èƒ½çš„ç‰¹æ®Šè¾“å…¥æƒ…å†µ
        b1 = src_boxes.unsqueeze(1)
        b2 = tgt_boxes.unsqueeze(0)

    # 2. æå–é«˜æ–¯å‚æ•°
    # å‡å€¼ mu: ä¸­å¿ƒç‚¹ (cx, cy)
    mu1 = b1[..., :2]
    mu2 = b2[..., :2]
    
    # æ ‡å‡†å·® sigma: å®½é«˜çš„ä¸€åŠ (w/2, h/2)ï¼Œä»£è¡¨ä¸ç¡®å®šæ€§èŒƒå›´
    # æ³¨æ„: è¿™é‡Œå‡è®¾è¾“å…¥æ¡†æ˜¯å½’ä¸€åŒ–åæ ‡(0-1)ã€‚
    sigma1 = b1[..., 2:] / 2.0
    sigma2 = b2[..., 2:] / 2.0

    # 3. è®¡ç®— Wasserstein è·ç¦»çš„å¹³æ–¹ (W2^2)
    # å…¬å¼: ||mu1 - mu2||^2 + ||sigma1 - sigma2||^2 (Frobenius norm for diagonal covariance)
    
    # ä½ç½®å·®å¼‚ (Location Discrepancy)
    xy_distance_sq = torch.sum((mu1 - mu2)**2, dim=-1)
    
    # å°ºåº¦/å½¢çŠ¶å·®å¼‚ (Scale/Shape Discrepancy)
    wh_distance_sq = torch.sum((sigma1 - sigma2)**2, dim=-1)
    
    # æ€» Wasserstein è·ç¦»
    w2_sq = xy_distance_sq + wh_distance_sq

    # 4. éçº¿æ€§æ˜ å°„: å°†è·ç¦»è½¬æ¢ä¸ºâ€œç›¸ä¼¼åº¦â€ (Gaussian Similarity)
    # ä½¿ç”¨æŒ‡æ•°æ ¸å‡½æ•°ã€‚tau æ˜¯æ¸©åº¦ç³»æ•°ï¼Œæ§åˆ¶å¯¹è·ç¦»çš„æ•æ„Ÿåº¦ã€‚
    # å¯¹äºå½’ä¸€åŒ–åæ ‡(0-1)ï¼Œå»ºè®® tau å–è¾ƒå°å€¼ (å¦‚ 0.05 - 0.1)ã€‚
    # è¿™é‡Œæˆ‘ä»¬å¼•å…¥ä¸€ä¸ªè‡ªé€‚åº”çš„å½’ä¸€åŒ–é¡¹ (sigma1*sigma2)ï¼Œä½¿å¾—åº¦é‡å¯¹å°ºåº¦ç›¸å¯¹ä¸æ•æ„Ÿ
    scale_term = (sigma1.prod(dim=-1).sqrt() + sigma2.prod(dim=-1).sqrt()) + eps
    tau = 0.1 
    # å½’ä¸€åŒ– Wasserstein Distance (NWD) çš„å˜ä½“
    gaussian_similarity = torch.exp(-w2_sq / (tau * scale_term + eps))

    # 5. è¾…åŠ©å‡ ä½•ç‰¹å¾ (ä¿ç•™æ–¹å‘æ€§)
    # å› ä¸ºé«˜æ–¯åˆ†å¸ƒæ˜¯å¯¹ç§°çš„ï¼Œä¼šä¸¢å¤±â€œAåœ¨Bå·¦è¾¹â€è¿™ç§æ–¹å‘ä¿¡æ¯ï¼Œæ‰€ä»¥éœ€è¦ä¿ç•™è§’åº¦ç‰¹å¾
    delta_xy = mu2 - mu1
    angle = torch.atan2(delta_xy[..., 1], delta_xy[..., 0])
    
    # 6. ä¼ ç»Ÿçš„ Log-space ç›¸å¯¹ç‰¹å¾ (ä½œä¸ºè¡¥å……ï¼Œä¿æŒç‰¹å¾ç»´åº¦ä¸º6)
    # ç›¸å¯¹ä½ç½®
    rel_xy = torch.abs(mu1 - mu2) / (sigma1 + eps) 
    rel_xy = torch.log(rel_xy + 1.0)
    # ç›¸å¯¹å°ºå¯¸
    rel_wh = torch.log((sigma1 + eps) / (sigma2 + eps))

    # === ç‰¹å¾èåˆ ===
    # è¾“å‡ºç»´åº¦: 1 (Sim) + 1 (Angle) + 2 (XY) + 2 (WH) = 6
    pos_embed = torch.cat([
        gaussian_similarity.unsqueeze(-1), 
        angle.unsqueeze(-1),
        rel_xy,
        rel_wh
    ], dim=-1) 
    
    return pos_embed
# def box_rel_encoding(src_boxes, tgt_boxes, eps=1e-5):
#     # construct position relation
#     xy1, wh1 = src_boxes.split([2, 2], -1)
#     xy2, wh2 = tgt_boxes.split([2, 2], -1)
#     delta_xy = torch.abs(xy1.unsqueeze(-2) - xy2.unsqueeze(-3))
#     delta_xy = torch.log(delta_xy / (wh1.unsqueeze(-2) + eps) + 1.0)
#     delta_wh = torch.log((wh1.unsqueeze(-2) + eps) / (wh2.unsqueeze(-3) + eps))
#     pos_embed = torch.cat([delta_xy, delta_wh], -1)  # [batch_size, num_boxes1, num_boxes2, 4]

#     return pos_embed
class PosEmbedding(nn.Module):
    # def __init__(self):
    def __init__(self, input_dim=6):
        super().__init__()
        self.linear1 = nn.Linear(6, 12)
        # self.linear1 = nn.Linear(input_dim, 12) # åŠ¨æ€è¾“å…¥ç»´åº¦
        self.linear2 = nn.Linear(12, 24)
        self.relu = nn.GELU()
    #     self._init_weights()

    # def _init_weights(self):
    #     for m in self.modules():
    #         if isinstance(m, nn.Linear):
    #             nn.init.xavier_uniform_(m.weight)
    #             if m.bias is not None:
    #                 nn.init.zeros_(m.bias)

    def forward(self, x):
        x = self.relu(self.linear1(x))
        return self.linear2(x)


class PositionRelationEmbedding(nn.Module):
    def __init__(
        self,
        embed_dim=256,
        num_heads=8,
        temperature=10000.0,
        scale=100.0,
        activation_layer=nn.ReLU,
        inplace=True,
    ):
        super().__init__()
        self.pos_proj = Conv2dNormActivation(
            embed_dim * 6,
            num_heads,
            kernel_size=1,
            inplace=inplace,
            norm_layer=None,
            activation_layer=activation_layer,
        )
        # self.pos_func = functools.partial(
        #     get_sine_pos_embed,
        #     num_pos_feats=embed_dim,
        #     temperature=temperature,
        #     scale=scale,
        #     exchange_xy=False,
        # )
        self.pos_func =PosEmbedding()

    # def forward(self, src_boxes: Tensor, tgt_boxes: Tensor = None):
    def forward(self, src_boxes: torch.Tensor, tgt_boxes: torch.Tensor = None):
        if tgt_boxes is None:
            tgt_boxes = src_boxes
        # # src_boxes: [batch_size, num_boxes1, 4]
        # # tgt_boxes: [batch_size, num_boxes2, 4]
        # torch._assert(src_boxes.shape[-1] == 4, f"src_boxes much have 4 coordinates")
        # torch._assert(tgt_boxes.shape[-1] == 4, f"tgt_boxes must have 4 coordinates")
        # with torch.no_grad():
        #     pos_embed = box_rel_encoding(src_boxes, tgt_boxes)
        #     pos_embed = self.pos_func(pos_embed).permute(0, 3, 1, 2)
        # pos_embed = self.pos_proj(pos_embed)
        with torch.no_grad():
            # ================= [INNOVATION START] =================
            # ä½¿ç”¨é«˜æ–¯å…³ç³»ç¼–ç  (Gaussian Relation) æ›¿ä»£åŸå§‹çš„ Box Relation
            # åŸå§‹: pos_embed = box_rel_encoding(src_boxes, tgt_boxes)
            # pos_embed = gaussian_relation_encoding(src_boxes, tgt_boxes)
            pos_embed = gcd_relation_encoding(src_boxes, tgt_boxes)
            # ================= [INNOVATION END] ===================
            
            # æ¥ä¸‹æ¥çš„ MLP æ˜ å°„ä¿æŒä¸å˜
            pos_embed = self.pos_func(pos_embed).permute(0, 3, 1, 2)
            
        pos_embed = self.pos_proj(pos_embed)

        return pos_embed.clone()

# class PositionRelationEmbedding(nn.Module):
#     def __init__(
#         self,
#         embed_dim=256,
#         num_heads=8,
#         variation='gcd', # æ–°å¢å‚æ•°: 'none', 'original', 'iou', 'gcd'
#         activation_layer=nn.ReLU,
#         inplace=True,
#     ):
#         super().__init__()
#         self.variation = variation.lower()
        
#         # ================= [æ‰“å°æç¤ºä¿¡æ¯ START] =================
#         print("-" * 50)
#         print(f" [GCPRE] Initializing Position Relation Module...")
#         print(f" [GCPRE] Current Mode: {self.variation.upper()}")
        
#         # 1. æ ¹æ®å˜ä½“ç¡®å®šè¾“å…¥ç‰¹å¾ç»´åº¦ (input_dim) å¹¶æ‰“å°å…·ä½“é…ç½®
#         if self.variation == 'original':   
#             self.input_dim = 4
#             print(" [GCPRE] Configuration: Variation B (Original Relation-DETR)")
#             print(" [GCPRE] Features: [delta_xy(2), delta_wh(2)]")
            
#         elif self.variation == 'iou':      
#             self.input_dim = 5
#             print(" [GCPRE] Configuration: Variation C (Original + IoU)")
#             print(" [GCPRE] Features: [delta_xy(2), delta_wh(2), iou(1)]")
            
#         elif self.variation == 'gcd':      
#             self.input_dim = 6             
#             print(" [GCPRE] Configuration: Variation D (Ours: GCDecoder)")
#             print(" [GCPRE] Features: [GCD_sim(1), Angle(1), Log_dist(4)]")
            
#         elif self.variation == 'none':     
#             self.input_dim = 0
#             print(" [GCPRE] Configuration: Variation A (RT-DETR Baseline)")
#             print(" [GCPRE] Status: Module DISABLED. Returning None.")
            
#         else:
#             raise ValueError(f"Unknown variation: {self.variation}")
            
#         print(f" [GCPRE] Input Dimension: {self.input_dim}")
#         print("-" * 50)
#         # ================= [æ‰“å°æç¤ºä¿¡æ¯ END] ===================

#         # 1. æ ¹æ®å˜ä½“ç¡®å®šè¾“å…¥ç‰¹å¾ç»´åº¦ (input_dim)
#         if self.variation == 'original':   # Variation B
#             self.input_dim = 4
#         elif self.variation == 'iou':      # Variation C
#             self.input_dim = 5
#         elif self.variation == 'gcd':      # Variation D (Ours)
#             self.input_dim = 6             # å¯¹åº” gcd_relation_encoding çš„è¾“å‡ºç»´åº¦
#         elif self.variation == 'none':     # Variation A
#             self.input_dim = 0
#         else:
#             raise ValueError(f"Unknown variation: {self.variation}")

#         # Variation A ä¸éœ€è¦åˆå§‹åŒ–åç»­ç½‘ç»œ
#         if self.variation != 'none':
#             # 2. åˆå§‹åŒ– MLPï¼Œä¼ å…¥å¯¹åº”çš„ input_dim
#             self.pos_func = PosEmbedding(input_dim=self.input_dim)

#             # 3. åˆå§‹åŒ–æŠ•å½±å±‚
#             # PosEmbedding çš„è¾“å‡ºå›ºå®šæ˜¯ 24 (Linear2çš„è¾“å‡º)ï¼Œä½†åç»­ä»£ç ä¼¼ä¹ä¾èµ– embed_dim
#             # åŸä»£ç é€»è¾‘: self.pos_proj æ¥æ”¶ embed_dim * 6 ?? 
#             # ä¿®æ­£: åŸä»£ç  self.pos_proj æ¥æ”¶çš„æ˜¯ pos_func çš„è¾“å‡ºã€‚
#             # ä½ çš„ PosEmbedding è¾“å‡ºæ˜¯ 24ï¼Œæ‰€ä»¥è¿™é‡Œ proj çš„è¾“å…¥åº”è¯¥æ˜¯ 24ã€‚
#             # ä½†å¦‚æœä½ çš„åŸæ„æ˜¯ pos_func è¾“å‡º embed_dim ç›¸å…³çš„ç»´åº¦ï¼Œè¯·è°ƒæ•´ PosEmbeddingã€‚
#             # å‡è®¾æŒ‰ç…§ä½ æä¾›çš„ PosEmbeddingï¼Œè¾“å‡ºæ˜¯ 24:
#             self.pos_proj = Conv2dNormActivation(
#                 24,  # è¿™é‡Œå¿…é¡»åŒ¹é… PosEmbedding çš„è¾“å‡ºç»´åº¦ (self.linear2 çš„ out_features)
#                 num_heads,
#                 kernel_size=1,
#                 inplace=inplace,
#                 norm_layer=None,
#                 activation_layer=activation_layer,
#             )

#     def forward(self, src_boxes: torch.Tensor, tgt_boxes: torch.Tensor = None):
#         # Variation A: RT-DETR Decoder (ä¸ä½¿ç”¨ä½ç½®å…³ç³»ç¼–ç )
#         if self.variation == 'none':
#             return None # æˆ–è€…è¿”å› torch.zeros(...)ï¼Œå–å†³äºä½ åœ¨ Decoder é‡Œçš„è°ƒç”¨æ–¹å¼

#         if tgt_boxes is None:
#             tgt_boxes = src_boxes
            
#         with torch.no_grad():
#             # æ ¹æ®é…ç½®é€‰æ‹©ç‰¹å¾è®¡ç®—å‡½æ•°
#             if self.variation == 'original':
#                 pos_embed = box_rel_encoding(src_boxes, tgt_boxes)
#             elif self.variation == 'iou':
#                 pos_embed = iou_rel_encoding(src_boxes, tgt_boxes)
#             elif self.variation == 'gcd':
#                 # ä½¿ç”¨ä½ æä¾›çš„ gcd å‡½æ•°
#                 # æ³¨æ„ï¼šç¡®ä¿ gcd_relation_encoding åœ¨æ­¤ä½œç”¨åŸŸå¯è§
#                 pos_embed = gcd_relation_encoding(src_boxes, tgt_boxes) 
            
#         # MLP æ˜ å°„ [BS, N, M, input_dim] -> [BS, N, M, 24]
#         pos_embed = self.pos_func(pos_embed)
        
#         # è°ƒæ•´ç»´åº¦ [BS, 24, N, M] ä»¥é€‚é… Conv2d
#         pos_embed = pos_embed.permute(0, 3, 1, 2)
        
#         # æŠ•å½±åˆ° Head ç»´åº¦ [BS, num_heads, N, M]
#         pos_embed = self.pos_proj(pos_embed)

#         return pos_embed.clone()


###
# class PositionRelationEmbedding(nn.Module):
#     def __init__(
#         self,
#         embed_dim=256,
#         num_heads=8,
#         temperature=10000.0,
#         scale=100.0,
#         activation_layer=nn.ReLU,
#         inplace=True,
#     ):
#         super().__init__()
        
#         # æ³¨æ„ï¼šè¿™é‡Œçš„ input_channels å–å†³äº MLP çš„è¾“å‡º
#         # ä½ çš„ä»£ç é‡Œå†™çš„æ˜¯ embed_dim * 6ï¼Œæˆ‘ä»¬ä¿æŒä¸€è‡´ï¼Œè®© MLP è¾“å‡ºè¿™ä¸ªç»´åº¦
#         proj_input_dim = embed_dim * 6
        
#         self.pos_proj = Conv2dNormActivation(
#             proj_input_dim, 
#             num_heads,
#             kernel_size=1,
#             inplace=inplace,
#             norm_layer=None,
#             activation_layer=activation_layer,
#         )
        
#         # ================= [INNOVATION 1: ç»“æ„å‡çº§] =================
#         # ä½¿ç”¨ FourierMLP æ›¿æ¢åŸæ¥çš„ PosEmbedding
#         # input_dim=7 (å¯¹åº” GCD ç¼–ç çš„ 7 ä¸ªé€šé“)
#         # output_dim=proj_input_dim (ç¡®ä¿å’Œ pos_proj å¯¹é½)
#         self.pos_func = FourierMLP(
#             input_dim=7, 
#             hidden_dim=embed_dim, 
#             output_dim=proj_input_dim,
#             sigma=20.0  # é’ˆå¯¹ UAV å¯†é›†åœºæ™¯æ¨èå€¼
#         )
#         # ================= [INNOVATION END] =======================

#     def forward(self, src_boxes: torch.Tensor, tgt_boxes: torch.Tensor = None):
#         if tgt_boxes is None:
#             tgt_boxes = src_boxes
            
#         with torch.no_grad():
#             # ================= [INNOVATION 2: å‡ ä½•å…ˆéªŒ] =================
#             # ä½¿ç”¨æˆ‘ä»¬åˆšæ‰å®šä¸‹æ¥çš„â€œç¨³å¥ç‰ˆ GCD ç¼–ç â€
#             # è¾“å‡ºç»´åº¦åº”è¯¥æ˜¯ 7 (1 Sim + 2 Angle + 4 LogRel)
#             pos_embed = gcd_relation_encoding(src_boxes, tgt_boxes)
#             # ================= [INNOVATION END] =========================
            
#             # ä½¿ç”¨ FourierMLP æ˜ å°„åˆ°é«˜ç»´
#             # [BS, N, M, 7] -> [BS, N, M, embed_dim*6]
#             pos_embed = self.pos_func(pos_embed)
            
#             # è°ƒæ•´ç»´åº¦ä»¥é€‚é…å·ç§¯å±‚: [BS, embed_dim*6, N, M]
#             pos_embed = pos_embed.permute(0, 3, 1, 2)
            
#         # å·ç§¯æŠ•å½±ï¼Œç”Ÿæˆæœ€ç»ˆçš„ Attention Bias
#         pos_embed = self.pos_proj(pos_embed)

#         return pos_embed.clone()





def gcd_relation_encoding(src_boxes, tgt_boxes, eps=1e-7):
    """
    åŸºäºè®ºæ–‡åŸæ–‡ä»£ç æ”¹ç¼–çš„ GCD å…³ç³»ç¼–ç  (Strict Implementation)
    
    é€»è¾‘æ¥æº: ç”¨æˆ·æä¾›çš„ gcd_loss å‡½æ•°
    æ ¸å¿ƒæ€æƒ³: åŒå‘ç›¸å¯¹è·ç¦»å¹³å‡ (Symmetrized Relative Distance)
    
    Args:
        src_boxes (Tensor): [BS, N, 4] (cx, cy, w, h)
        tgt_boxes (Tensor): [BS, M, 4] (cx, cy, w, h)
    Returns:
        pos_embed (Tensor): [BS, N, M, 6]
    """
    # 1. ç»´åº¦å¯¹é½ä¸å¹¿æ’­ (Broadcasting)
    # src: [BS, N, 1, 4], tgt: [BS, 1, M, 4]
    if src_boxes.dim() == 3:
        b1 = src_boxes.unsqueeze(2)  
        b2 = tgt_boxes.unsqueeze(1)
    else:
        b1 = src_boxes.unsqueeze(1)
        b2 = tgt_boxes.unsqueeze(0)

    # 2. æå–å‚æ•° (å‡è®¾è¾“å…¥å·²ç»æ˜¯ cx, cy, w, h æ ¼å¼)
    # center1, center2
    cx1, cy1, w1, h1 = b1.unbind(-1)
    cx2, cy2, w2, h2 = b2.unbind(-1)

    # 3. è®¡ç®—åŸºç¡€å·®å€¼
    # whs (center distance): dx, dy
    dx = cx1 - cx2
    dy = cy1 - cy2
    
    # 4. æŒ‰ç…§åŸæ–‡é€»è¾‘è®¡ç®—å››é¡¹è·ç¦»
    
    # --- Part 1: Relative to Box 1 (src) ---
    # center_distance1 = (dx/w1)^2 + (dy/h1)^2
    c_dist1 = (dx / (w1 + eps))**2 + (dy / (h1 + eps))**2
    
    # wh_distance2 (æ³¨æ„: åŸä»£ç ä¸­ wh_distance2 åˆ†æ¯æ˜¯ w1/h1)
    # wh_dist_relative_to_1 = ((w1-w2)/w1)^2 + ((h1-h2)/h1)^2
    # åŸæ–‡ä»£ç  wh_distance2 è¿˜æœ‰ä¸ª /4
    wh_dist1 = (((w1 - w2) / (w1 + eps))**2 + ((h1 - h2) / (h1 + eps))**2) / 4.0

    # --- Part 2: Relative to Box 2 (tgt) ---
    # center_distance2 = (dx/w2)^2 + (dy/h2)^2
    c_dist2 = (dx / (w2 + eps))**2 + (dy / (h2 + eps))**2
    
    # wh_distance1 (æ³¨æ„: åŸä»£ç ä¸­ wh_distance1 åˆ†æ¯æ˜¯ w2/h2)
    # wh_dist_relative_to_2 = ((w1-w2)/w2)^2 + ((h1-h2)/h2)^2
    wh_dist2 = (((w1 - w2) / (w2 + eps))**2 + ((h1 - h2) / (h2 + eps))**2) / 4.0

    # 5. ç»„åˆ GCD (Squared)
    # gcd_2 = (center_distance1 + wh_distance1 + center_distance2 + wh_distance2) / 2
    # æ³¨æ„å¯¹åº”å…³ç³»ï¼šåŸä»£ç çš„å˜é‡åä¸‹æ ‡å’Œåˆ†æ¯ä¸‹æ ‡æ˜¯äº¤å‰çš„ï¼Œè¿™é‡Œæˆ‘ä»¬ç›´æ¥åŠ æ€»å¹³å‡å³å¯
    gcd_2 = (c_dist1 + wh_dist1 + c_dist2 + wh_dist2) / 2.0

    # 6. è½¬æ¢ä¸ºç›¸ä¼¼åº¦ (Similarity Mode)
    # å¯¹åº”åŸä»£ç : if mode == 'exp': gcd = torch.exp(-torch.sqrt(gcd_2))
    # è¿™æ˜¯ä¸€ä¸ª 0~1 çš„å€¼ï¼Œéå¸¸é€‚åˆä½œä¸º Attention çš„æƒé‡æˆ–ç‰¹å¾
    gcd_similarity = torch.exp(-torch.sqrt(gcd_2 + eps))

    # 7. ä¿ç•™è¾…åŠ©å‡ ä½•ç‰¹å¾ (è§’åº¦ + ç›¸å¯¹ä½ç½®)
    # å› ä¸º GCD æ˜¯å¯¹ç§°æ ‡é‡ï¼Œä¸¢å¤±äº†æ–¹å‘ä¿¡æ¯ï¼Œæ‰€ä»¥å¿…é¡»ä¿ç•™ Angle
    # angle = torch.atan2(dy, dx)
    angle = torch.atan2(dy, dx)   # å½’ä¸€åŒ–åˆ° -1 åˆ° 1 ä¹‹é—´
    
    # ä¼ ç»Ÿçš„ Log-space ç›¸å¯¹ç‰¹å¾ (ä½œä¸ºè¡¥å……)
    # ä¾ç„¶ä¿ç•™ï¼Œç»™ MLP æä¾›åŸå§‹çš„ç›¸å¯¹ä½ç½®ä¿¡æ¯
    rel_x = torch.log(torch.abs(dx) / (w1 + eps) + 1.0)
    rel_y = torch.log(torch.abs(dy) / (h1 + eps) + 1.0)
    # rel_x = torch.sign(dx) * torch.log(torch.abs(dx) / (w1 + eps) + 1.0)
    # rel_y = torch.sign(dy) * torch.log(torch.abs(dy) / (h1 + eps) + 1.0) # è¿™é‡Œçš„é€»è¾‘æ˜¯ï¼šä¿æŒæ•°å€¼ä¹Ÿæ˜¯ log ç¼©æ”¾çš„ï¼Œä½†æ˜¯ä¿ç•™æ­£è´Ÿå·
    rel_w = torch.log((w1 + eps) / (w2 + eps))
    rel_h = torch.log((h1 + eps) / (h2 + eps))

    # === ç‰¹å¾èåˆ (6ç»´) ===
    # 1(GCD Sim) + 1(Angle) + 4(Rel Log Coords)
    pos_embed = torch.cat([
        gcd_similarity.unsqueeze(-1), 
        angle.unsqueeze(-1),
        rel_x.unsqueeze(-1), 
        rel_y.unsqueeze(-1),
        rel_w.unsqueeze(-1),
        rel_h.unsqueeze(-1)
    ], dim=-1) 
    
    return pos_embed


# def gcd_relation_encoding(src_boxes, tgt_boxes, eps=1e-7):
#     # 1. ç»´åº¦å¯¹é½ (ä¿æŒä¸å˜)
#     if src_boxes.dim() == 3:
#         b1 = src_boxes.unsqueeze(2)
#         b2 = tgt_boxes.unsqueeze(1)
#     else:
#         b1 = src_boxes.unsqueeze(1)
#         b2 = tgt_boxes.unsqueeze(0)

#     # 2. æå–å‚æ•° (ä¿æŒä¸å˜)
#     cx1, cy1, w1, h1 = b1.unbind(-1)
#     cx2, cy2, w2, h2 = b2.unbind(-1)

#     # 3. è®¡ç®—åŸºç¡€å·®å€¼ (ä¿æŒä¸å˜)
#     dx = cx1 - cx2
#     dy = cy1 - cy2

#     # 4. è®¡ç®— GCD è·ç¦» (ä¿æŒä¸å˜ï¼Œè¿™æ˜¯æ ¸å¿ƒé€»è¾‘)
#     # Part 1
#     c_dist1 = (dx / (w1 + eps))**2 + (dy / (h1 + eps))**2
#     wh_dist1 = (((w1 - w2) / (w1 + eps))**2 + ((h1 - h2) / (h1 + eps))**2) / 4.0
#     # Part 2
#     c_dist2 = (dx / (w2 + eps))**2 + (dy / (h2 + eps))**2
#     wh_dist2 = (((w1 - w2) / (w2 + eps))**2 + ((h1 - h2) / (h2 + eps))**2) / 4.0
#     # Average
#     gcd_2 = (c_dist1 + wh_dist1 + c_dist2 + wh_dist2) / 2.0

#     # ================= [å…³é”®ä¿®æ”¹ç‚¹ START] =================
    
#     # [ä¿®æ”¹ 1] å¼•å…¥æ¸©åº¦ç³»æ•° tauï¼Œé˜²æ­¢åˆæœŸæ¢¯åº¦æ¶ˆå¤±
#     # ä»é›¶è®­ç»ƒå»ºè®® tau=2.0 æˆ– 3.0
#     tau = 2.0  
#     gcd_similarity = torch.exp(-torch.sqrt(gcd_2 + eps) / tau)

#     # [ä¿®æ”¹ 2] è§’åº¦è¿ç»­åŒ– (Sin/Cos)
#     # è§£å†³ -pi åˆ° pi çš„çªå˜éš¾è®­ç»ƒé—®é¢˜
#     raw_angle = torch.atan2(dy, dx)
#     sin_angle = torch.sin(raw_angle)
#     cos_angle = torch.cos(raw_angle)

#     # [ä¿®æ”¹ 3] Log ç‰¹å¾ç¼©æ”¾
#     # ä¹˜ä»¥ 0.2 å°†èŒƒå›´ä» 0~5 å‹åˆ° 0~1 å·¦å³ï¼Œé¿å…åœ¨åˆæœŸä¸»å¯¼ MLP
#     scale_factor = 0.2
#     rel_x = torch.log(torch.abs(dx) / (w1 + eps) + 1.0) * scale_factor
#     rel_y = torch.log(torch.abs(dy) / (h1 + eps) + 1.0) * scale_factor
#     rel_w = torch.log((w1 + eps) / (w2 + eps)) * scale_factor
#     rel_h = torch.log((h1 + eps) / (h2 + eps)) * scale_factor

#     # ================= [å…³é”®ä¿®æ”¹ç‚¹ END] =================

#     # ç‰¹å¾èåˆ (æ³¨æ„ï¼šç°åœ¨æ˜¯ 7 ç»´äº†ï¼)
#     # 1(Sim) + 1(Sin) + 1(Cos) + 4(Rel) = 7
#     pos_embed = torch.cat([
#         gcd_similarity.unsqueeze(-1), 
#         sin_angle.unsqueeze(-1),
#         cos_angle.unsqueeze(-1),
#         rel_x.unsqueeze(-1), 
#         rel_y.unsqueeze(-1),
#         rel_w.unsqueeze(-1),
#         rel_h.unsqueeze(-1)
#     ], dim=-1) 
    
#     return pos_embed


# def gcd_relation_encoding_linear(src_boxes, tgt_boxes, eps=1e-7):
#     """
#     Formula B: Linear-Symmetric GCD Implementation
    
#     ç‰¹ç‚¹:
#     1. å®Œå…¨å¯¹ç§°: A->B å’Œ B->A çš„ç‰¹å¾äº’ä¸ºç›¸åæ•°æˆ–ç›¸åŒã€‚
#     2. çº¿æ€§ç©ºé—´: ç›¸å¯¹ä½ç½®ä½¿ç”¨çº¿æ€§é™¤æ³• (dx / joint_sigma)ï¼Œè€Œé Logã€‚
#     3. æ•°å€¼é£é™©: è¿œè·ç¦»ç‰©ä½“çš„ç‰¹å¾å€¼å¯èƒ½éå¸¸å¤§ (>100)ï¼Œå¯èƒ½å¯¼è‡´æ¢¯åº¦çˆ†ç‚¸ã€‚
    
#     Args:
#         src_boxes (Tensor): [BS, N, 4]
#         tgt_boxes (Tensor): [BS, M, 4]
#     """
#     # 1. ç»´åº¦å¯¹é½ä¸å¹¿æ’­
#     if src_boxes.dim() == 3:
#         b1 = src_boxes.unsqueeze(2)  
#         b2 = tgt_boxes.unsqueeze(1)
#     else:
#         b1 = src_boxes.unsqueeze(1)
#         b2 = tgt_boxes.unsqueeze(0)

#     # 2. æå–å‚æ•°
#     cx1, cy1, w1, h1 = b1.unbind(-1)
#     cx2, cy2, w2, h2 = b2.unbind(-1)

#     # 3. è®¡ç®—åŸºç¡€å·®å€¼
#     dx = cx1 - cx2
#     dy = cy1 - cy2
    
#     # ================= [Formula B æ ¸å¿ƒé€»è¾‘] =================
    
#     # 4. è®¡ç®—å¯¹ç§°è”åˆæ–¹å·® (Symmetric Joint Variance)
#     # è¿™æ˜¯ Formula B çš„æ ¸å¿ƒï¼šåˆ†æ¯èåˆäº†ä¸¤ä¸ªç‰©ä½“çš„å°ºå¯¸
#     # var = w1^2 + w2^2
#     joint_var_w = w1.pow(2) + w2.pow(2) + eps
#     joint_var_h = h1.pow(2) + h2.pow(2) + eps
    
#     # å¯¹åº”çš„è”åˆæ ‡å‡†å·® (ç”¨äºçº¿æ€§ç‰¹å¾å½’ä¸€åŒ–)
#     # sigma = sqrt(w1^2 + w2^2)
#     joint_std_w = torch.sqrt(joint_var_w)
#     joint_std_h = torch.sqrt(joint_var_h)

#     # 5. è®¡ç®— GCD ç»¼åˆè·ç¦» (Squared)
#     # ä½ç½®é¡¹: dx^2 / (w1^2 + w2^2)
#     term_loc = (dx.pow(2) / joint_var_w) + (dy.pow(2) / joint_var_h)
    
#     # å½¢çŠ¶é¡¹: (w1-w2)^2 / (w1^2 + w2^2)
#     term_shape = ((w1 - w2).pow(2) / joint_var_w) + ((h1 - h2).pow(2) / joint_var_h)
    
#     # æ€»è·ç¦»
#     gcd_dist = term_loc + term_shape

#     # ================= [ç‰¹å¾å‘é‡æ„å»º] =================

#     # Feature 1: GCD ç›¸ä¼¼åº¦ (0~1)
#     # è¿™ä¸€é¡¹æ˜¯å®‰å…¨çš„ï¼Œæœ‰ exp å‹åˆ¶
#     f_similarity = torch.exp(-torch.sqrt(gcd_dist))
    
#     # Feature 2: ç›¸å¯¹è§’åº¦ (-pi ~ pi)
#     # è¿™ä¸€é¡¹ä¹Ÿæ˜¯å®‰å…¨çš„
#     f_angle = torch.atan2(dy, dx)
    
#     # Feature 3 & 4: å¯¹ç§°çº¿æ€§ç›¸å¯¹åæ ‡ (Symmetric Linear Relative Coords)
#     # [è­¦å‘Š]!!! è¿™ä¸€é¡¹å°±æ˜¯ Formula B çš„é£é™©æº
#     # å¦‚æœä¸¤ä¸ªå°ç‰©ä½“ç›¸è·å¾ˆè¿œï¼Œè¿™ä¸ªå€¼ä¼šéå¸¸å¤§ (ä¾‹å¦‚ 1900 / 14 = 135.7)
#     f_rel_x = dx / joint_std_w
#     f_rel_y = dy / joint_std_h
    
#     # Feature 5 & 6: ç›¸å¯¹å®½é«˜ (Log)
#     # ä¿æŒ Log å½¢å¼ä»¥ç»´æŒåå¯¹ç§°æ€§ log(w1/w2)
#     f_rel_w = torch.log((w1 + eps) / (w2 + eps))
#     f_rel_h = torch.log((h1 + eps) / (h2 + eps))

#     # 6. ç‰¹å¾æ‹¼æ¥
#     pos_embed = torch.cat([
#         f_similarity.unsqueeze(-1),
#         f_angle.unsqueeze(-1),
#         f_rel_x.unsqueeze(-1),
#         f_rel_y.unsqueeze(-1),
#         f_rel_w.unsqueeze(-1),
#         f_rel_h.unsqueeze(-1)
#     ], dim=-1) 
    
#     return pos_embed